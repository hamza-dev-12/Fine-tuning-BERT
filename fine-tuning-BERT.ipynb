{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3428111,"sourceType":"datasetVersion","datasetId":2066095}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Dependencies**","metadata":{}},{"cell_type":"code","source":"!pip install accelerate\n!pip install bitsandbytes\n!pip install trl\n!pip install peft","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preparing Dataset**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/twitter-tweets-sentiment-dataset/Tweets.csv')\ndf = df.dropna()\n\ntext = df['selected_text']\nsentiment = df['sentiment']\n\nprint('Dataset Lenght: ',len(text))\n\ntrain_size = int(0.9 * len(text))\nval_size = len(text) - train_size\nprint('training size: ',train_size)\nprint('val_size: ', val_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:39:56.645847Z","iopub.execute_input":"2024-07-12T10:39:56.646531Z","iopub.status.idle":"2024-07-12T10:39:58.214397Z","shell.execute_reply.started":"2024-07-12T10:39:56.646500Z","shell.execute_reply":"2024-07-12T10:39:58.212993Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Dataset Lenght:  27480\ntraining size:  24732\nval_size:  2748\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ds = df[:train_size].reset_index(drop=True)\nval_ds = df[train_size:].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:40:03.163235Z","iopub.execute_input":"2024-07-12T10:40:03.164038Z","iopub.status.idle":"2024-07-12T10:40:03.171321Z","shell.execute_reply.started":"2024-07-12T10:40:03.163998Z","shell.execute_reply":"2024-07-12T10:40:03.170203Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def compute_ratio(ds):\n    texts = ds['selected_text']\n    sentiments = ds['sentiment']\n    print(len(texts[0]))\n    max_len = 0\n    min_len = 90000\n    \n    positives = 0\n    negatives = 0\n    neutral = 0\n    \n    for i in range(len(ds)):\n        \n        if sentiments[i] == 'positive':\n            positives = positives + 1\n            \n        elif sentiments[i] == 'negative':\n            negatives = negatives + 1\n            \n        else:\n            neutral = neutral + 1\n        \n        max_len = max(max_len, len(texts[i]))\n        min_len = min(min_len, len(texts[i]))\n        \n    return positives , negatives, neutral, max_len, min_len\n\n\nprint(compute_ratio(train_ds))\nprint(compute_ratio(val_ds))\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:41:05.532343Z","iopub.execute_input":"2024-07-12T10:41:05.532683Z","iopub.status.idle":"2024-07-12T10:41:06.021376Z","shell.execute_reply.started":"2024-07-12T10:41:05.532656Z","shell.execute_reply":"2024-07-12T10:41:06.020349Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"35\n(7795, 6987, 9950, 139, 1)\n15\n(787, 794, 1167, 141, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Logging to Hugging Face**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-07-08T11:35:12.301735Z","iopub.execute_input":"2024-07-08T11:35:12.302208Z","iopub.status.idle":"2024-07-08T11:35:12.694537Z","shell.execute_reply.started":"2024-07-08T11:35:12.302164Z","shell.execute_reply":"2024-07-08T11:35:12.693374Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4f81e3d92a7437e9e9914ba208b57ee"}},"metadata":{}}]},{"cell_type":"code","source":"sentiment_mapping = {\"positive\": 0, \"negative\": 1, \"neutral\": 2}\ntrain_ds['sentiment'] = [sentiment_mapping[sent] for sent in train_ds['sentiment']]\nval_ds['sentiment'] = [sentiment_mapping[sent] for sent in val_ds['sentiment']]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:41:23.249875Z","iopub.execute_input":"2024-07-12T10:41:23.250677Z","iopub.status.idle":"2024-07-12T10:41:23.273366Z","shell.execute_reply.started":"2024-07-12T10:41:23.250647Z","shell.execute_reply":"2024-07-12T10:41:23.272519Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom datasets import Dataset\nimport torch\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntrain_ds = Dataset.from_pandas(train_ds)\nval_ds = Dataset.from_pandas(val_ds)\n\n\ndef tokenize_function(examples):\n    tokens =  tokenizer(examples[\"selected_text\"], padding=\"max_length\", truncation=True , return_tensors='pt')\n\n    # Convert each tensor in the dictionary to float16\n    for key in tokens.keys():\n        tokens[key] = tokens[key].to(dtype=torch.float16 if tokens[key].dtype.is_floating_point else tokens[key].dtype)\n\n    return tokens\n\n\ntokenized_train_ds = train_ds.map(tokenize_function)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:41:27.930748Z","iopub.execute_input":"2024-07-12T10:41:27.931526Z","iopub.status.idle":"2024-07-12T10:42:00.552475Z","shell.execute_reply.started":"2024-07-12T10:41:27.931495Z","shell.execute_reply":"2024-07-12T10:42:00.551560Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61f824536a34abe8c0242d715a43b3f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b97401e332464eea9c766f50529cd744"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f164b897421949809d3d39cc4afd373f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e3d992579347238b5e27efe7d8bc1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0f584182a4043f08b57c03e22292481"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_train_ds['sentiment'][:10]","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:42:11.306213Z","iopub.execute_input":"2024-07-12T10:42:11.306962Z","iopub.status.idle":"2024-07-12T10:42:11.328499Z","shell.execute_reply.started":"2024-07-12T10:42:11.306932Z","shell.execute_reply":"2024-07-12T10:42:11.327621Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[2, 1, 1, 1, 1, 2, 0, 2, 2, 0]"},"metadata":{}}]},{"cell_type":"markdown","source":"**LORA Configuration**","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:44:12.368708Z","iopub.execute_input":"2024-07-12T10:44:12.369455Z","iopub.status.idle":"2024-07-12T10:44:14.768506Z","shell.execute_reply.started":"2024-07-12T10:44:12.369420Z","shell.execute_reply":"2024-07-12T10:44:14.767672Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\n\n# Initialize the accelerator\naccelerator = Accelerator(mixed_precision=\"fp16\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:44:20.508185Z","iopub.execute_input":"2024-07-12T10:44:20.508760Z","iopub.status.idle":"2024-07-12T10:44:20.544804Z","shell.execute_reply.started":"2024-07-12T10:44:20.508730Z","shell.execute_reply":"2024-07-12T10:44:20.543995Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Importing model**","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-cased',\n    num_labels=3,\n)\n\nmodel = accelerator.prepare(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:44:24.710578Z","iopub.execute_input":"2024-07-12T10:44:24.711340Z","iopub.status.idle":"2024-07-12T10:44:27.703923Z","shell.execute_reply.started":"2024-07-12T10:44:24.711307Z","shell.execute_reply":"2024-07-12T10:44:27.702855Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"075ee5c882c14897ab1690e937324d08"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import get_peft_model\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:44:40.447607Z","iopub.execute_input":"2024-07-12T10:44:40.447961Z","iopub.status.idle":"2024-07-12T10:44:40.508643Z","shell.execute_reply.started":"2024-07-12T10:44:40.447932Z","shell.execute_reply":"2024-07-12T10:44:40.507749Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def count_trainable_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad) , sum(p.numel() for p in model.parameters() if 1)\n\n# Assuming 'model' is your original PyTorch model\ntrainable_parameters , original_parameters = count_trainable_params(model)\n\nprint('Trainable parameters are: ',trainable_parameters/original_parameters * 100,'%')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:44:44.861968Z","iopub.execute_input":"2024-07-12T10:44:44.862647Z","iopub.status.idle":"2024-07-12T10:44:44.874217Z","shell.execute_reply.started":"2024-07-12T10:44:44.862612Z","shell.execute_reply":"2024-07-12T10:44:44.873358Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Trainable parameters are:  0.03615170036478414 %\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:44:47.067544Z","iopub.execute_input":"2024-07-12T10:44:47.067902Z","iopub.status.idle":"2024-07-12T10:44:47.084273Z","shell.execute_reply.started":"2024-07-12T10:44:47.067873Z","shell.execute_reply":"2024-07-12T10:44:47.083418Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(28996, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=768, out_features=3, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=768, out_features=3, bias=True)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n# class_weights = torch.tensor([0.1,0.2,0.1,0.2,0.1,0.4]).to('cuda')\n\ndef compute_loss(model, inputs, attention_mask, labels):\n    \n    inputs = inputs.view(inputs.size(0),inputs.size(-1)).to('cuda')\n#     labels = labels.view(labels.size(0),labels.size(-1)).to('cuda')\n    attention_mask = attention_mask.view(attention_mask.size(0),attention_mask.size(-1)).to('cuda')\n    \n    outputs = model(inputs,attention_mask)\n    criterion = nn.CrossEntropyLoss()\n    loss = criterion(outputs.logits, labels)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-07-10T09:24:52.517492Z","iopub.execute_input":"2024-07-10T09:24:52.517790Z","iopub.status.idle":"2024-07-10T09:24:52.526595Z","shell.execute_reply.started":"2024-07-10T09:24:52.517766Z","shell.execute_reply":"2024-07-10T09:24:52.525799Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"code","source":"def rename_sentiment_to_labels(examples):\n    examples[\"labels\"] = examples.pop(\"sentiment\")\n    return examples\n\n# Apply the renaming function to the datasets\ntokenized_train_ds = tokenized_train_ds.map(rename_sentiment_to_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:45:02.296255Z","iopub.execute_input":"2024-07-12T10:45:02.296603Z","iopub.status.idle":"2024-07-12T10:45:02.540375Z","shell.execute_reply.started":"2024-07-12T10:45:02.296575Z","shell.execute_reply":"2024-07-12T10:45:02.539252Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7afd9f5c5eca4de99c1269e300e55a59"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n                                 num_train_epochs=25,)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T08:45:43.609056Z","iopub.execute_input":"2024-07-09T08:45:43.609914Z","iopub.status.idle":"2024-07-09T08:45:43.652788Z","shell.execute_reply.started":"2024-07-09T08:45:43.609880Z","shell.execute_reply":"2024-07-09T08:45:43.651871Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"##custom training loop\nimport torch\n\ndef custom_training_loop():\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n    model.train()\n\n    for epoch in range(training_args.num_train_epochs):\n        total_loss = 0.0\n\n        data_loader = trainer.get_train_dataloader()\n\n        for index , batch in enumerate(data_loader):\n            inputs = batch['input_ids'] \n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n\n            # Calculate loss\n            loss = compute_loss(model, inputs, attention_mask, labels)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            total_loss += loss.item()\n            if index % 500 == 0:\n              print(index, ' of ',len(data_loader))\n\n        print(f\"Epoch {epoch + 1}: Average Loss: {total_loss / len(trainer.get_train_dataloader())}\")\ncustom_training_loop()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T08:47:05.956145Z","iopub.execute_input":"2024-07-09T08:47:05.956647Z","iopub.status.idle":"2024-07-09T12:28:25.457659Z","shell.execute_reply.started":"2024-07-09T08:47:05.956613Z","shell.execute_reply":"2024-07-09T12:28:25.456452Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 1: Average Loss: 0.8313468009194921\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 2: Average Loss: 0.5378294685207317\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 3: Average Loss: 0.44587951873032733\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 4: Average Loss: 0.41555936075747013\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 5: Average Loss: 0.4011515626469374\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 6: Average Loss: 0.38791713268673206\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 7: Average Loss: 0.38112071677223136\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 8: Average Loss: 0.37045793814818345\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 9: Average Loss: 0.36312796546804105\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 10: Average Loss: 0.3567399597771341\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 11: Average Loss: 0.350613738454753\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 12: Average Loss: 0.3495743663868304\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 13: Average Loss: 0.34342835818020984\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 14: Average Loss: 0.3399757630809814\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 15: Average Loss: 0.334191355820538\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 16: Average Loss: 0.3327297768200161\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 17: Average Loss: 0.33034788666806275\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 18: Average Loss: 0.32389459642129836\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 19: Average Loss: 0.32471896751384993\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 20: Average Loss: 0.3189966707149422\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 21: Average Loss: 0.3147002171151542\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 22: Average Loss: 0.31466589844333825\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 23: Average Loss: 0.31166976832219656\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 24: Average Loss: 0.30527428178331134\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 25: Average Loss: 0.3033000683896292\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Saving Model**","metadata":{}},{"cell_type":"code","source":"epoch = 25\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\nPATH = '/kaggle/working/bert_model.pth'\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n#     'loss': loss,\n#     'lora_state_dict': model.lora.state_dict(),  # Assuming the LoRA adapter is accessible via `model.lora`\n}, PATH)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T12:43:20.989733Z","iopub.execute_input":"2024-07-09T12:43:20.990080Z","iopub.status.idle":"2024-07-09T12:43:21.600596Z","shell.execute_reply.started":"2024-07-09T12:43:20.990055Z","shell.execute_reply":"2024-07-09T12:43:21.599584Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"**Inference**","metadata":{}},{"cell_type":"code","source":"import torch\nsentence = val_ds['selected_text'][10]\ntokenized_sentence = tokenizer(sentence,return_tensors=\"pt\").to('cuda')\nprint('sentence is: ',sentence)\n\nwith torch.no_grad():\n    outputs = model(**tokenized_sentence)\n\n# The model outputs a tuple, the first element is the logits\nlogits = outputs.logits\nprint(logits)\n# Apply softmax to get probabilities\nprobabilities = torch.nn.functional.softmax(logits, dim=-1)\n\nprint(probabilities)\nemotion = ['positive','negative','neutral']\n\nmax_index = torch.argmax(probabilities[0])\nprint(max_index.item())\nprint('actual_label: ',val_ds['sentiment'][10])","metadata":{"execution":{"iopub.status.busy":"2024-07-08T14:39:57.511984Z","iopub.execute_input":"2024-07-08T14:39:57.512330Z","iopub.status.idle":"2024-07-08T14:39:57.548959Z","shell.execute_reply.started":"2024-07-08T14:39:57.512304Z","shell.execute_reply":"2024-07-08T14:39:57.548013Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"sentence is:  sorry I missed you at the farm today,\ntensor([[-2.1660,  1.2646, -0.5708]], device='cuda:0')\ntensor([[0.0272, 0.8390, 0.1339]], device='cuda:0')\n1\nactual_label:  1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Testing Dataset Accuracy**","metadata":{}},{"cell_type":"code","source":"prediction = []\nfor i in range(len(val_ds)):\n    \n    sentence = val_ds['selected_text'][i]\n    tokenized_sentence = tokenizer(sentence,return_tensors=\"pt\").to('cuda')\n    \n    with torch.no_grad():\n        outputs = model(**tokenized_sentence)\n        \n    logits = outputs.logits\n    # Apply softmax to get probabilities\n    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n\n\n    max_index = torch.argmax(probabilities[0])\n    prediction.append(max_index)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:29:11.320833Z","iopub.execute_input":"2024-07-12T10:29:11.321261Z","iopub.status.idle":"2024-07-12T10:30:26.654150Z","shell.execute_reply.started":"2024-07-12T10:29:11.321228Z","shell.execute_reply":"2024-07-12T10:30:26.653317Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2024-07-12 10:29:15.618774: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 10:29:15.618883: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 10:29:15.873350: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"correct_outcomes = 0\n\nfor i in range(len(prediction)):\n    if prediction[i] == val_ds['sentiment'][i]:\n        correct_outcomes += 1\n        \nprint('Accuracy: ',correct_outcomes/len(prediction))","metadata":{"execution":{"iopub.status.busy":"2024-07-10T09:29:52.465078Z","iopub.execute_input":"2024-07-10T09:29:52.465748Z","iopub.status.idle":"2024-07-10T09:30:33.579391Z","shell.execute_reply.started":"2024-07-10T09:29:52.465718Z","shell.execute_reply":"2024-07-10T09:30:33.578395Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Accuracy:  0.9057496360989811\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Loading my Finetuned Model**","metadata":{}},{"cell_type":"code","source":"from accelerate import Accelerator\n\n# Initialize the accelerator\naccelerator = Accelerator(mixed_precision=\"fp16\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:26:25.272148Z","iopub.execute_input":"2024-07-12T10:26:25.272612Z","iopub.status.idle":"2024-07-12T10:26:26.829426Z","shell.execute_reply.started":"2024-07-12T10:26:25.272556Z","shell.execute_reply":"2024-07-12T10:26:26.828406Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-cased',\n    num_labels=3,\n)\n\nmodel = accelerator.prepare(model)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:26:29.174667Z","iopub.execute_input":"2024-07-12T10:26:29.175570Z","iopub.status.idle":"2024-07-12T10:26:32.625734Z","shell.execute_reply.started":"2024-07-12T10:26:29.175535Z","shell.execute_reply":"2024-07-12T10:26:32.624993Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8861956e65314d27bc988006463548a7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:28:04.897539Z","iopub.execute_input":"2024-07-12T10:28:04.897910Z","iopub.status.idle":"2024-07-12T10:28:06.168866Z","shell.execute_reply.started":"2024-07-12T10:28:04.897879Z","shell.execute_reply":"2024-07-12T10:28:06.168134Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from peft import get_peft_model\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:28:08.836204Z","iopub.execute_input":"2024-07-12T10:28:08.836560Z","iopub.status.idle":"2024-07-12T10:28:08.899667Z","shell.execute_reply.started":"2024-07-12T10:28:08.836531Z","shell.execute_reply":"2024-07-12T10:28:08.898986Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\noptimizer = torch.optim.Adam(model.parameters(), lr=5e-5)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:45:39.815959Z","iopub.execute_input":"2024-07-12T10:45:39.816591Z","iopub.status.idle":"2024-07-12T10:45:39.823110Z","shell.execute_reply.started":"2024-07-12T10:45:39.816560Z","shell.execute_reply":"2024-07-12T10:45:39.822243Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:28:20.166003Z","iopub.execute_input":"2024-07-12T10:28:20.166358Z","iopub.status.idle":"2024-07-12T10:28:20.182802Z","shell.execute_reply.started":"2024-07-12T10:28:20.166328Z","shell.execute_reply":"2024-07-12T10:28:20.182009Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(28996, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSdpaSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.1, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=1, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=1, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=768, out_features=3, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=768, out_features=3, bias=True)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\ncheckpoint = torch.load('/kaggle/working/bert_model.pth')\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:45:43.677402Z","iopub.execute_input":"2024-07-12T10:45:43.678314Z","iopub.status.idle":"2024-07-12T10:45:44.064366Z","shell.execute_reply.started":"2024-07-12T10:45:43.678272Z","shell.execute_reply":"2024-07-12T10:45:44.063380Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds['labels'][:10]","metadata":{"execution":{"iopub.status.busy":"2024-07-10T09:26:18.809854Z","iopub.execute_input":"2024-07-10T09:26:18.810518Z","iopub.status.idle":"2024-07-10T09:26:18.832300Z","shell.execute_reply.started":"2024-07-10T09:26:18.810486Z","shell.execute_reply":"2024-07-10T09:26:18.831307Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[2, 1, 1, 1, 1, 2, 0, 2, 2, 0]"},"metadata":{}}]},{"cell_type":"markdown","source":"**Continue Training**","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n                                 num_train_epochs=5,)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:45:52.598484Z","iopub.execute_input":"2024-07-12T10:45:52.598855Z","iopub.status.idle":"2024-07-12T10:46:06.351072Z","shell.execute_reply.started":"2024-07-12T10:45:52.598827Z","shell.execute_reply":"2024-07-12T10:46:06.350143Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"2024-07-12 10:45:55.310479: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-12 10:45:55.310598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-12 10:45:55.502035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n# class_weights = torch.tensor([0.1,0.2,0.1,0.2,0.1,0.4]).to('cuda')\n\ndef compute_loss(model, inputs, attention_mask, labels):\n    \n    inputs = inputs.view(inputs.size(0),inputs.size(-1)).to('cuda')\n#     labels = labels.view(labels.size(0),labels.size(-1)).to('cuda')\n    attention_mask = attention_mask.view(attention_mask.size(0),attention_mask.size(-1)).to('cuda')\n    \n    outputs = model(inputs,attention_mask)\n    criterion = nn.CrossEntropyLoss()\n    loss = criterion(outputs.logits, labels)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:46:27.730422Z","iopub.execute_input":"2024-07-12T10:46:27.731346Z","iopub.status.idle":"2024-07-12T10:46:27.739397Z","shell.execute_reply.started":"2024-07-12T10:46:27.731305Z","shell.execute_reply":"2024-07-12T10:46:27.738303Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"##custom training loop\nimport torch\n\ndef custom_training_loop():\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5) #decayed 5 times\n    model.train()\n\n    for epoch in range(training_args.num_train_epochs):\n        total_loss = 0.0\n\n        data_loader = trainer.get_train_dataloader()\n\n        for index , batch in enumerate(data_loader):\n            inputs = batch['input_ids'] \n            attention_mask = batch['attention_mask']\n            labels = batch['labels']\n\n            # Calculate loss\n            loss = compute_loss(model, inputs, attention_mask, labels)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            total_loss += loss.item()\n            if index % 500 == 0:\n              print(index, ' of ',len(data_loader))\n\n        print(f\"Epoch {epoch + 1}: Average Loss: {total_loss / len(trainer.get_train_dataloader())}\")\ncustom_training_loop()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T10:46:49.823678Z","iopub.execute_input":"2024-07-12T10:46:49.824134Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 1: Average Loss: 0.3014703040332913\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 2: Average Loss: 0.3000309510580607\n0  of  1546\n500  of  1546\n1000  of  1546\n1500  of  1546\nEpoch 3: Average Loss: 0.29783447505845984\n0  of  1546\n500  of  1546\n1000  of  1546\n","output_type":"stream"}]}]}